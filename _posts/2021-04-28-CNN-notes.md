---
title: "Deep Learning Notes"
categories:
  - blog
tags:
  - Machine Learning
  - Neural Network
---
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
A generalization of linear discriminant

$$ y = g(w^Tx +x_0) $$
where g is a monotonic activation function

$$ y(x) = g(a) $$

For linear regression
$$ g(a) = a $$

$$ a^n = w^Tx^n = \sum_{i=0}^{d}w_ix_i^n $$

Mean Squared Error:

$$ MSE = \frac{1}{N} \sum_{n=1}^{N} (t^n -y^n)^2 $$

Gradient Descent in general

$$ w_i = w_i - \alpha \frac{\partial COST}{\partial w_i} $$

Batch learning rule: 
$$ w_i = w_i + \alpha \frac{1}{N} \sum_{n=1}^{N} (t^n -y^n)x_i^n \$$

Delta(error) rule:
$$ w_i = w_i + \alpha \delta^n x)i^n $$

Activations:
Softmax:
$$a_k = w_k^Tx$$
$$y_k = g(a_k) = \frac{e^{a_k}}{\sum_{j=1}{c} e^{a_j}} $$

ReLu:
$$y=g(a) = max(a,0)$$

Tanh:
$$y=g(a) = tanh(a) $$


Gaussian Distribution -> Sum Squared Error
Bernouli Distribution -> Cross Entropy
Multinomial Distribution -> (One Hot Encoding)

$$\sum_{n=1}^{N} \sum{k=1}^{c} t_k^n ln y_k^n $$

Characteristic of Perceptron Learning:

- supervised learning: gives set of input output examples to model the data
- error correction learning: only correct it when it's wrong
- random presentation of patterns
- slow, learning some pattern might affect others

Back propagation: 
learns internal representation (features)
change the connecting strengths according to the error
generalize to recurrent network

learning rule:
$$ w_{ij} = w_{ij} - \alpha \frac{\partial J}{\partial w_{ij}} = 
w_{ij} + \alpha \delta_j z_i $$

\\(\delta_j = (t_j - y_j)\\) for output unit (assume correct objective function and output activation function)

\\(\delta_j = g'(a_j)\sum_{k} \delta_k w_{jk}\\) for hidden layer

Examples:(Network learns features in the service of the task)

NetTalk: learn to read text
Hinton's Family Tree: learn to represent relationships between people, 
learn distributed representation of concepts(localist(one-hot-encoding))

Tricks of the Trade

- Stochastic/Full Batch/ Mini-Batch Gradient Descent
Stochastic gradient descent: select random subset to perform gradient descent
- faster iteration, lower convergence rate
- tend to get better solution
- adapt to changing environment
- can be used easily for online feature

Batch learning:
- ?Since 2012 new online approaches have been developed

- Example shuffling
guide: present examples with more errors more often

- Perform PCA on the input
mean cancelation : shift to 0

Change sigmoid to tanh to have positive and negative output

f(X) = 1.7159xtanh(2/3x) 

slope changing fastest at +/-1

hidden layer activation choice
1,2 layer: tanh
more layers: relu





